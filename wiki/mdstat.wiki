= Mdstat =

<h6>Reading</h6>

[http://wiki.centos.org/HowTos/SoftwareRAIDonCentOS5 http://wiki.centos.org/HowTos/SoftwareRAIDonCentOS5]

[http://tldp.org/HOWTO/Software-RAID-HOWTO-6.html http://tldp.org/HOWTO/Software-RAID-HOWTO-6.html]

[https://raid.wiki.kernel.org/articles/m/d/s/Mdstat.html https://raid.wiki.kernel.org/articles/m/d/s/Mdstat.html]

[http://radu.rendec.ines.ro/howto/raid1.html http://radu.rendec.ines.ro/howto/raid1.html]

<h6>Automation</h6>

Actually we monitor the status via cron or nagios using a script as follows

<pre>
root@biz5 ~ # crontab -l 
MAILTO=evanx@ipay.co.za
4 4 * * * /opt/scripts/rdiff-backup.sh
9 9 * * * /opt/scripts/check_mdstat.sh
</pre>

Where our monitoring script grep's /proc/mdstat psuedo file for underscores.

<pre>
root@biz5 ~ # cat /scripts/check_mdstat.sh 

cat /proc/mdstat | grep -q "_"
if [ $? -eq 0 ]
then
  dmesg | grep "error\|sda\|sdb\|disk"
  mdstat /dev/md2 --detail
  cat /proc/mdstat 
  cat /proc/mdstat | mail -s "mdstat disk error" evanx@ipay.co.za
fi
</pre>

; Inspect

Check the partition and md setup.

<pre>
root@biz5 ~ # cat /etc/fstab 
proc /proc proc defaults 0 0
none /dev/pts devpts gid=5,mode=620 0 0
/dev/md0 none swap sw 0 0
/dev/md1 /boot ext3 defaults 0 0
/dev/md2 / ext3 defaults 0 0

root@biz5 ~ # df
Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/md2             966683064   5829652 912133896   1% /
/dev/md1                256586     23076    220262  10% /boot
</pre>

Check the status via /proc/mdstat psuedo file. 

<pre>
root@biz5 ~ # cat /proc/mdstat 
Personalities : [raid1] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sda1[0] sdb1[1]
      2102464 blocks [2/2] [UU]
      
md1 : active raid1 sda2[0] sdb2[1]
      264960 blocks [2/2] [UU]
      
md2 : active raid1 sdb3[1]
      974390336 blocks [2/1] [_U]
      
unused devices: <none>
</pre>

Underscores in the following are faulty partitions e.g. sda3, which has been removed i.e. sda3.

Before hot removing a partition, we might see "sda3[0](F)" when detected as faulty because of I/O errors etc.

Manually check for I/O errors.

<pre>
root@biz5 ~ # dmesg | grep -i "disk\|error"
VFS: Disk quotas dquot_6.5.1
md: md driver 0.90.3 MAX_MD_DEVS=256, MD_SB_DISKS=27
sd 0:0:0:0: Attached scsi disk sda
sd 1:0:0:0: Attached scsi disk sdb
md: cannot remove active disk sda1 from md0 ... 
raid1: Disk failure on sda1, disabling device. 
</pre>

; Remove and re-add failed partitions 

In order to attempt to rebuild a partition, we can remove and re-add.

<pre>
mdadm /dev/md2 --remove /dev/sda3
mdadm /dev/md2 --add /dev/sda3
</pre>

; Remove faulty drive partitions

<pre>
root@biz5 ~ # mdadm /dev/md0 --remove /dev/sda1
mdadm: hot remove failed for /dev/sda1: Device or resource busy
</pre>

We might need to --set_faulty to remove the swap partition.

<pre>
root@biz5 ~ # mdadm /dev/md0 --set-faulty /dev/sda1
mdadm: set /dev/sda1 faulty in /dev/md0
</pre>

Remove all the partitions of the faulty drive to be physically removed.

<pre>
root@biz5 ~ # mdadm /dev/md0 --remove /dev/sda1
root@biz5 ~ # mdadm /dev/md1 --remove /dev/sda2
root@biz5 ~ # mdadm /dev/md2 --remove /dev/sda3
</pre>

<pre>
root@biz5 ~ # cat /proc/mdstat 
Personalities : [raid1] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdb1[1]
      2102464 blocks [2/1] [_U]
      
md1 : active raid1 sdb2[1]
      264960 blocks [2/1] [_U]
      
md2 : active raid1 sdb3[1]
      974390336 blocks [2/1] [_U]
</pre>

; Serial number

Check the serial number of the drive to remove.
<pre>
hdparm -i /dev/sda
</pre>

; Grub 

Ensure that grub is installed on the secondary drive so when the primary one is removed, the system still boots.

<pre>
root@biz5 ~ # cat /boot/grub/device.map
(hd0)   /dev/sda
(hd1)   /dev/sdb
</pre>

One can use grub interactive shell to install boot onto both RAID1 drives, as follows.

<pre>
root@biz5 ~ # grub
grub> device (hd0) /dev/sda
device (hd0) /dev/sda
grub> root (hd0,1) 
root (hd0,1)
 Filesystem type is ext2fs, partition type 0xfd
grub> setup (hd0)
setup (hd0)
</pre>

<pre>
root@biz5 ~ # grub
grub> device (hd1) /dev/sdb
device (hd1) /dev/sdb
grub> root (hd1,1) 
root (hd1,1)
 Filesystem type is ext2fs, partition type 0xfd
grub> setup (hd1)
setup (hd1)
</pre>

Incidently if the /dev/sda is faulty, the following would also work, referring to /dev/sdb as hd0.

<pre>
root@biz5 ~ # grub
grub> device (hd0) /dev/sdb
grub> root (hd0,1) 
root (hd0,1)
 Filesystem type is ext2fs, partition type 0xfd
grub> setup (hd0)
setup (hd0)
</pre>

However when the new drive is inserted, we must install grub into its MBR as well i.e. both hd0 and hd1.

; grub-install

On CentOS (RHEL, Fedora), one can use grub-install, rather than the above interactive shell, grub.

<pre>
root@biz5 /home/evanx # vi /boot/grub/grub.conf
timeout 5
default 0

title CentOS (2.6.18-274.7.1.el5)
root (hd1,1)
kernel /vmlinuz-2.6.18-274.7.1.el5 ro root=/dev/md2 vga=0x317 selinux=0
initrd /initrd-2.6.18-274.7.1.el5.img

root@biz5 /home/evanx # /sbin/grub-install /dev/sdb
</pre>

where we changed the root device hd1 (where /boot resides) in grub.conf and then run grub-install to install grub in the MBR of that device.


